{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "import numpy as np\n",
    "from pyspark.sql.types import Row\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History logging enabled\n"
     ]
    }
   ],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "%azureml history on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-76f5bf5c8bf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"false\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wasb://data-files@kpmgstorage1.blob.core.windows.net/traindata/*.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"false\").csv(\"wasb://data-files@kpmgstorage1.blob.core.windows.net/traindata/*.csv\")\n",
    "df.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, we need to transform data into one hot vectors\n",
    "These steps are:\n",
    "    1. Find unique values\n",
    "    2. Create a list which contains an index and value, which can be used for one hot encoding.\n",
    "    3. One hot encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0_unique = df.select('_c0').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "c1_unique = df.select('_c1').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "c5_unique = df.select('_c5').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "c6_unique = df.select('_c6').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "print(c0_unique)\n",
    "print(len(c0_unique))\n",
    "print(c1_unique)\n",
    "print(len(c1_unique))\n",
    "print(c5_unique)\n",
    "print(len(c5_unique))\n",
    "print(c6_unique)\n",
    "print(len(c6_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example creating one hot encoding for something with 7 classes for class 3\n",
    "print(np.eye(7)[3])\n",
    "\n",
    "#Example applying this to our data\n",
    "print(np.eye(len(c1_unique))[c1_unique.index('Sat')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now to do this to a spark data frame\n",
    "\n",
    "#Define Function to make this easier to deal with\n",
    "def One_Hot_String(val_list, val, sep=' '):\n",
    "    array = np.eye(len(val_list))[val_list.index(val)].astype(int)\n",
    "    return sep.join(str(i) for i in array.tolist())\n",
    "\n",
    "#Apply Function to each column\n",
    "mapped = df.rdd.map(lambda r: ( Row(One_Hot_String(c0_unique, r['_c0'])), \n",
    "                                r['_c1'], \n",
    "                                r['_c2'], \n",
    "                                r['_c3'],\n",
    "                                r['_c4'],\n",
    "                                r['_c5'],\n",
    "                                r['_c6'])).toDF()\n",
    "\n",
    "mapped = mapped.rdd.map(lambda r: ( r['_1'], \n",
    "                                Row(One_Hot_String(c1_unique, r['_2'])), \n",
    "                                r['_3'], \n",
    "                                r['_4'],\n",
    "                                r['_5'],\n",
    "                                r['_6'],\n",
    "                                r['_7'])).toDF()\n",
    "\n",
    "mapped = mapped.rdd.map(lambda r: ( r['_1'], \n",
    "                                r['_2'], \n",
    "                                r['_3'], \n",
    "                                r['_4'],\n",
    "                                r['_5'],\n",
    "                                Row(One_Hot_String(c5_unique, r['_6'])),\n",
    "                                r['_7'])).toDF()\n",
    "\n",
    "mapped = mapped.rdd.map(lambda r: ( r['_1'], \n",
    "                                r['_2'], \n",
    "                                r['_3'], \n",
    "                                r['_4'],\n",
    "                                r['_5'],\n",
    "                                r['_6'],\n",
    "                                Row(One_Hot_String(c6_unique, r['_7'])) )).toDF()\n",
    "\n",
    "mapped.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Label File\n",
    "Deep learning frameworks typically operate on label files.  These are completely processed files in a format specific to that framework which the framework can derive optimal performance from.  We are going to create a label file for CNTK.\n",
    "\n",
    "The type of label file we will create is a CTF File or CNTK Text File.  These follow the format:\n",
    "\n",
    "|somedescriptor value1 value2 value3 |somedescriptor2 value1 value2 value3\n",
    "\n",
    "In our case, we will build a file of the form:\n",
    "\n",
    "|features _1 _2 _3 _4 _5 _6 |label _7\n",
    "\n",
    "|features _1 _2 _3 _4 _5 _6 |label _7\n",
    "\n",
    "|features _1 _2 _3 _4 _5 _6 |label _7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_ctf_string(r):\n",
    "    s = '|label ' + str(r['_7'][0]) + ' '\n",
    "    s = s + '|features ' + str(r['_1'][0]) + ' '\n",
    "    s = s + str(r['_2'][0]) + ' '\n",
    "    s = s + str(r['_3'][0]) + ' '\n",
    "    s = s + str(r['_4'][0]) + ' '\n",
    "    s = s + str(r['_5'][0]) + ' '\n",
    "    s = s + str(r['_6'][0]) + ' '\n",
    "    return s\n",
    "pfw = mapped.rdd.flatMap(lambda r: Row(row_to_ctf_string(r)) )\n",
    "pfw.saveAsTextFile(\"wasb://data-files@bikesharestorage.blob.core.windows.net/train_ctf\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "BikeShareSample local",
   "language": "python",
   "name": "bikesharesample_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
